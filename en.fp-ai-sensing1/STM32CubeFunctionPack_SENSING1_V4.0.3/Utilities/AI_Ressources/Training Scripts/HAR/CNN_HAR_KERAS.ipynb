{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8 \n",
    "\n",
    "#   This software component is licensed by ST under BSD 3-Clause license,\n",
    "#   the \"License\"; You may not use this file except in compliance with the\n",
    "#   License. You may obtain a copy of the License at:\n",
    "#                        https://opensource.org/licenses/BSD-3-Clause\n",
    "  \n",
    "\n",
    "'''\n",
    "Training script of human activity recognition system (HAR), based on two different Convolutional Neural Network (CNN) architectures \n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step HAR Training STM32CubeAI\n",
    "This notebook provides a step by step demonstration of a simple <u>H</u>uman <u>A</u>ctivity <u>R</u>ecognition system (HAR), based on a convolutional networks (CNN). This script provides a simple data preperation script through `DataHelper` class and let user to preprocess, split, and segment the dataset to bring it into the form which can be used for training and validation of the HAR CNN. It also has a `CNNHandler` class which builds, trains and validate a CNN for a given set of input and output tensors. The `CNNHandler` can create one of the two provided CNN architectures namely, **IGN** and **GMP**.\n",
    "\n",
    "All the implementations are done in Python using [Keras](https:keras.io/) with [Tensorflow](https://www.tensorflow.org/) as backend.\n",
    "\n",
    "For demonstration purposes this script uses two datasets created for HAR using accelerometer sensor. \n",
    "\n",
    "* WISDM, a public dataset provided by <u>WI</u>reless <u>S</u>ensing <u>D</u>ata <u>M</u>ining group. The details of the dataset are available [here](http://www.cis.fordham.edu/wisdm/dataset.php).\n",
    "\n",
    "* AST our own propritery dataset.\n",
    "\n",
    "**Note**: We are not providing any dataset in the function pack. The user can download WISDM dataset from [here](http://www.cis.fordham.edu/wisdm/dataset.php), while AST is a private dataset and is not provided.\n",
    "\n",
    "Following figure shows the detailed workflow of HAR.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"760\" height=\"400\" src=\"workflow.png\">\n",
    "</p>\n",
    "\n",
    "Let us start the implementation now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 : Import necessary dependencies\n",
    "Following section imports all the required dependencies. This also sets seeds for random number generators in Numpy and Tensorflow environments to make the results reproducibile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(611)\n",
    "\n",
    "import argparse, os, logging, warnings\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "\n",
    "# private libraries\n",
    "from PrepareDataset import DataHelper\n",
    "from HARNN import ANNModelHandler\n",
    "\n",
    "# for using callbacks to save the model during training and comparing the results at every epoch\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# disabling annoying warnings originating from Tensorflow\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(611)\n",
    "\n",
    "# disabling annoying warnings originating from python\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Set environment variables\n",
    "Following section sets some user variables which will later be used for:\n",
    "\n",
    "* preparing the dataset.\n",
    "* preparing the neural networks.\n",
    "* training the neural networks.\n",
    "* validating the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data variables\n",
    "dataset = 'WISDM'\n",
    "merge = True\n",
    "segmentLength = 24\n",
    "stepSize = 24\n",
    "dataDir = 'datasets/ai_logged_data'\n",
    "preprocessing = True\n",
    "\n",
    "# neural network variables\n",
    "modelName = 'IGN'\n",
    "\n",
    "# training variables\n",
    "trainTestSplit = 0.6\n",
    "trainValidationSplit = 0.7\n",
    "nEpochs = 20\n",
    "learningRate = 0.0005\n",
    "decay = 1e-6\n",
    "batchSize = 64\n",
    "verbosity = 1\n",
    "nrSamplesPostValid = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Result directory\n",
    "Each run can have different variables and to compare the results of different choices, such as different segment size for the window for data, different overlap settings etc, we need to save the results. Following section creates a result directory to save results for the current run. The name of the directory has following format. `Mmm_dd_yyyy_hh_mm_ss_dataset_model_seqLen_stepSize_epochs_results`, and example name for directory can be `Oct_24_2019_14_31_20_WISDM_IGN_24_16_20_results`, which shows the process was started at October 24, 2019, at 14:31:20, the dataset used was WISDM, with segment size = 24, segment step = 16, and Nr of epochs = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already exist create a parent directory for results.\n",
    "if not os.path.exists( './results/'):\n",
    "    os.mkdir( './results/' )\n",
    "resultDirName = 'results/{}/'.format(datetime.now().strftime( \"%Y_%b_%d_%H_%M_%S\" ) )\n",
    "os.mkdir( resultDirName )\n",
    "infoString = 'runTime : {}\\nDatabase : {}\\nNetwork : {}\\nSeqLength : {}\\nStepSize : {}\\nEpochs : {}\\n'.format( datetime.now().strftime(\"%Y-%b-%d at %H:%M:%S\"), dataset, modelName, segmentLength, stepSize, nEpochs )\n",
    "with open( resultDirName + 'info.txt', 'w' ) as text_file:\n",
    "    text_file.write( infoString )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Create a `DataHelper` object\n",
    "The script in the following section creates a `DataHelper` object to preprocess, segment and split the dataset as well as to create one-hot-code labeling for the outputs to make the data training and testing ready using the choices set by the user in **Step2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper = DataHelper( dataset = dataset, loggedDataDir = dataDir, merge = merge,\n",
    "                            modelName = modelName, seqLength = segmentLength, seqStep = stepSize,\n",
    "                            preprocessing = preprocessing, trainTestSplit = trainTestSplit,\n",
    "                            trainValidSplit = trainValidationSplit, resultDir = resultDirName )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Prepare the dataset\n",
    "Following section prepares the dataset and create six tensors namely `TrainX`, `TrainY`, `ValidationX`, `ValidationY`, `TestX`, `TestY`. Each of the variables with trailing `X` are the inputs with shape `[_, segmentLength, 3, 1 ]`and each of the variables with trailing `Y` are corresponding outputs with shape `[ _, NrClasses ]`. `NrClasses` for `WISDM` can be `4` or `6` and for `AST` is `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX, TrainY, ValidationX, ValidationY, TestX, TestY = myDataHelper.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Create a `ANNModelHandler` object\n",
    "The script in the following section creates a `ANNModelHandler` object to create, train and validate the <u>A</u>rtificial <u>N</u>eural <u>N</u>etwork (ANN) using the variables created in **Step2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHarHandler = ANNModelHandler( modelName = modelName, classes = myDataHelper.classes, resultDir = resultDirName,\n",
    "                              inputShape = TrainX.shape, outputShape = TrainY.shape, learningRate = learningRate,\n",
    "                              decayRate = decay, nEpochs = nEpochs, batchSize = batchSize,\n",
    "                              modelFileName = 'har_' + modelName, verbosity = verbosity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Create a ANN model\n",
    "Following script creates the ANN and prints its summary to show the architecture and nr of parameters for ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harModel = myHarHandler.build_model()\n",
    "harModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7: Create a Checkpoint for ANN training\n",
    "The following script creates a check point for the training process of ANN to save the neural network as `h5` file. The settings are used in a way that the validation accuracy `val_acc` is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harModelCheckPoint = ModelCheckpoint( filepath = join(resultDirName, 'har_' + modelName + '.h5'),\n",
    "                                     monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'max' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 : Train the created neural network\n",
    "The following script trains the created neural network with the provided checkpoint and created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harModel = myHarHandler.train_model( harModel, TrainX, TrainY, ValidationX, ValidationY, harModelCheckPoint )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8: Validating the trained neural network\n",
    "The following section validates the created network and creates a confusion matrix for the test dataset to have a detailed picture of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHarHandler.make_confusion_matrix(  harModel, TestX, TestY )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9: Create an npz file for validation after conversion from CubeAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper.dump_data_for_post_validation( TestX, TestY, nrSamplesPostValid )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
